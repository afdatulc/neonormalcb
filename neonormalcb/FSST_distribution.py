# -*- coding: utf-8 -*-
"""Source code cdf pdf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f0O2ftIG18EItcae0edzPm9cGp7Ooow5

# PENGEMBANGAN DAN PENGUJIAN

## PENGEMBANGAN

### Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd drive/My Drive/Colab Notebooks/neonormal

import pymc as pm
import numpy as np
import arviz as az
import xarray as xr
import pytensor.tensor as pt
import matplotlib.pyplot as plt
import scipy.stats as st
import math
import tensorflow_probability as tfp
import tensorflow as tf
from pytensor.compile.ops import as_op
import scipy

from pytensor.tensor import TensorVariable
from typing import Optional, Tuple
from pymc.distributions.dist_math import check_parameters
from pymc.distributions.shape_utils import rv_size_is_none

!ls

# import sys
# sys.path.append('/content/drive/My Drive/Colab Notebooks/neonormal')
# import import_ipynb
# from FSST_distribution import fsst

sampler_kwargs = {
    "chains": 4,
    "cores": 4,
    "return_inferencedata": True,
    "random_seed": 42,
}

"""#PENGEMBANGAN"""

import pytensor.tensor.random as rnd

tfd = tfp.distributions


@as_op(itypes=[pt.dscalar, pt.dscalar, pt.dscalar, pt.dscalar, pt.dscalar], otypes=[pt.dscalar])
def tfp_logpdf(y, df, loc, scale, skewness):
    return np.array(tfd.TwoPieceStudentT(df=df, loc=loc, scale=scale, skewness=skewness).log_prob(y), dtype=np.float64)

def logp(y: TensorVariable, mu: TensorVariable, sigma: TensorVariable, nu: TensorVariable, alpha: TensorVariable, **kwargs):
    logpdf = tfp_logpdf(y, nu, mu, sigma, alpha)

    return check_parameters(
        logpdf,
        sigma > 0,
        nu > 0,
        alpha > 0,
        msg="sigma, nu, and alpha must be positive"
    )


@as_op(itypes=[pt.dscalar, pt.dscalar, pt.dscalar, pt.dscalar, pt.dscalar], otypes=[pt.dscalar])
def tfp_logcdf(y, df, loc, scale, skewness):
    return np.array(tfd.TwoPieceStudentT(df=df, loc=loc, scale=scale, skewness=skewness).log_cdf(y), dtype=np.float64)

def logcdf(y: TensorVariable, mu: TensorVariable, sigma: TensorVariable, nu: TensorVariable, alpha: TensorVariable, **kwargs):
    logcdf = tfp_logcdf(y, nu, mu, sigma, alpha)

    return check_parameters(
        logcdf,
        sigma > 0,
        nu > 0,
        alpha > 0,
        msg="sigma, nu, and alpha must be positive"
    )


@as_op(itypes=[pt.dscalar, pt.dscalar, pt.dscalar, pt.dscalar, pt.dscalar], otypes=[pt.dscalar])
def tfp_quantile(p, df, loc, scale, skewness):
    hasil = tfd.TwoPieceStudentT(df=df, loc=loc, scale=scale, skewness=skewness).quantile(p)
    return np.array(hasil, dtype=np.float64)

# def icdf(p: TensorVariable, mu: TensorVariable, sigma: TensorVariable, nu: TensorVariable, alpha: TensorVariable, **kwargs):
#     qtl = tfp_quantile(p, nu, mu, sigma, alpha)

#     return check_parameters(
#         qtl,
#         sigma > 0,
#         nu > 0,
#         alpha > 0,
#         msg="sigma, nu, and alpha must be positive"
#     )

def random(
      mu: np.ndarray | float,
      sigma: np.ndarray | float,
      nu: np.ndarray | float,
      alpha: np.ndarray | float,
      rng: np.random.Generator = None,
      size: Optional[int | Tuple[int, ...]] = None,
      # size: Optional[Tuple[int]]=None,
    ):
    if sigma <= 0:
        raise ValueError("sigma must be positive")
    if nu <= 0:
        raise ValueError("nu must be positive")
    if alpha <= 0:
        raise ValueError("alpha must be positive")

    # Pastikan size tidak None
    size = size or ()

    # Inisialisasi RNG jika tidak disediakan
    if rng is None:
        rng = np.random.default_rng()

    dist = tfd.TwoPieceStudentT(df=nu, loc=mu, scale=sigma, skewness=alpha)
    samples = dist.sample(sample_shape=size).numpy()
    return np.asarray(samples)

    # u = rng.uniform(low=0, high=1, size=size)
    # dist = tfd.TwoPieceStudentT(df=nu, loc=mu, scale=sigma, skewness=alpha)
    # samples = dist.quantile(u).numpy()

    # return samples


    # # Generate uniform random numbers
    # u = rng.uniform(low=0, high=1, size=size)

    # # Use the quantile function to transform uniform samples
    # samples = tfp_quantile(u, nu, mu, sigma, alpha)

    # return np.asarray(samples)

class fsst:
    def __new__(self, name: str, mu, sigma, nu, alpha, observed=None, **kwargs):
        return pm.CustomDist(
            name,
            mu, sigma, nu, alpha,
            logp=logp,
            logcdf=logcdf,
            random=random,
            observed=observed,
            **kwargs
        )

    @classmethod
    def dist(cls, mu, sigma, nu, alpha, **kwargs):
        return pm.CustomDist.dist(
            mu, sigma, nu, alpha,
            logp=logp,
            logcdf=logcdf,
            random=random
        )


    @staticmethod
    def icdf(p, mu, sigma, nu, alpha):
        return tfp_quantile(p, nu, mu, sigma, alpha)

"""## PENGUJIAN

### Uji Coba Program

#### PDF
"""

mu = pt.scalar('mu')
sigma = pt.scalar('sigma')
alpha = pt.scalar('alpha')
nu = pt.scalar('nu')
value = pt.scalar('value')

rv = fsst.dist(mu=mu, sigma=sigma, nu=nu, alpha=alpha)
rv_logp = pm.logp(rv, value)

# Use .eval() for debugging
p=(rv_logp.eval({value: 0.9, mu: 0.0, sigma:1, nu: 1, alpha:2.0}))
print(math.exp(p))

# Compile a function for repeated evaluations
rv_logp_fn = pm.compile_pymc([value, mu, sigma, nu, alpha], rv_logp)
q = rv_logp_fn(value=-np.inf, mu=0, sigma=1, nu=15, alpha=1)
print(math.exp(q))

# Parameter distribusi
mu = 0
sigma = 1
nu = 1  # Nilai default nu
x = np.linspace(-5, 5, 100)  # Rentang data

#Beda sigma
# y1 = st.norm.pdf(x, mu, sigma)
# y2 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=1.0, nu=1.0, alpha=1.0) for xi in x])
# y3 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=2.0, nu=1.0, alpha=1) for xi in x])
# y4 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=3.0, nu=1.0, alpha=1) for xi in x])
# y5 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=0.8, nu=1.0, alpha=1.0) for xi in x])

#Beda alpha/kemencengan
y1 = st.norm.pdf(x, mu, sigma)
y2 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=1.0, nu=1.0, alpha=1.0) for xi in x])
y3 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=1.0, nu=1.0, alpha=0.6) for xi in x])
y4 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=1.0, nu=1.0, alpha=0.8) for xi in x])
y5 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=1.0, nu=1.0, alpha=2.0) for xi in x])

#Beda nu dengan kemencengan = 1
# y1 = st.norm.pdf(x, mu, sigma)
# y2 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=1.0, nu=0.6, alpha=1.0) for xi in x])
# y3 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=1.0, nu=1.0, alpha=1.0) for xi in x])
# y4 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=1.0, nu=2.0, alpha=1.0) for xi in x])
# y5 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=1.0, nu=3.0, alpha=1.0) for xi in x])

#Beda nu dengan menceng kanan/skewness = 2
# y1 = st.norm.pdf(x, mu, sigma)
# y2 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=1.0, nu=0.6, alpha=2.0) for xi in x])
# y3 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=1.0, nu=1.0, alpha=2.0) for xi in x])
# y4 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=1.0, nu=2.0, alpha=2.0) for xi in x])
# y5 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=1.0, nu=3.6, alpha=2.0) for xi in x])

#Beda nu dengan menceng kanan/skewness = 0.6
# y1 = st.norm.pdf(x, mu, sigma)
# y2 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=1.0, nu=0.6, alpha=0.6) for xi in x])
# y3 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=1.0, nu=1.0, alpha=0.6) for xi in x])
# y4 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=1.0, nu=2.0, alpha=0.6) for xi in x])
# y5 = np.exp([rv_logp_fn(value=xi, mu=0.0, sigma=1.0, nu=3.0, alpha=0.6) for xi in x])

# Plot hasil uji PDF FSST
plt.figure(figsize=(12, 6))
plt.plot(x, y1, label="Normal(0, 1)", color="orange", linestyle="dashed")
plt.plot(x, y2, label="FSST(0, 1, 1, 1)", color="blue", linewidth=1.0)
plt.plot(x, y3, label="FSST(0, 1, 1, 0.6)", color="green", linewidth=1.0)
plt.plot(x, y4, label="FSST(0, 1, 1, 0.8)", color="purple", linewidth=1.0)
plt.plot(x, y5, label="FSST(0, 1, 1, 2.0)", color="red", linewidth=1.0)

plt.xlabel("x")
plt.ylabel("Density")
plt.legend(loc="upper left")
plt.title("FSST PDF dengan Berbagai Nilai Alpha")
plt.show()

"""#### CDF"""

mu = pt.scalar('mu')
sigma = pt.scalar('sigma')
alpha = pt.scalar('alpha')
nu = pt.scalar('nu')
value = pt.scalar('value')

rv = fsst.dist(mu=mu, sigma=sigma, nu=nu, alpha=alpha)
rv_logcdf = pm.logcdf(rv, value)

# Use .eval() for debugging
p=(rv_logcdf.eval({value: 0.9, mu: 0.0, sigma:1, nu: 1, alpha:2.0}))
print(math.exp(p))

# Compile a function for repeated evaluations
rv_logcdf_fn = pm.compile_pymc([value, mu, sigma, nu, alpha], rv_logcdf)
q = rv_logcdf_fn(value=-np.inf, mu=0, sigma=1, nu=15, alpha=1)
print(math.exp(q))

# Parameter distribusi
mu = 0
sigma = 1
nu = 1  # Nilai default nu
x = np.linspace(-4, 4, 100)  # Rentang data

# Menghitung PDF distribusi FSST untuk berbagai nilai alpha
y1 = st.norm.cdf(x, mu, sigma)
y2 = np.exp([rv_logcdf_fn(value=xi, mu=0.0, sigma=1.0, nu=1.0, alpha=1.0) for xi in x])
y3 = np.exp([rv_logcdf_fn(value=xi, mu=0.0, sigma=1.0, nu=1.0, alpha=0.6) for xi in x])
y4 = np.exp([rv_logcdf_fn(value=xi, mu=0.0, sigma=1.0, nu=1.0, alpha=0.8) for xi in x])
y5 = np.exp([rv_logcdf_fn(value=xi, mu=0.0, sigma=1.0, nu=1.0, alpha=2.0) for xi in x])

# Plot hasil uji PDF FSST
plt.figure(figsize=(12, 6))
plt.plot(x, y1, label="Normal(0, 1)", color="orange", linestyle="dashed")
plt.plot(x, y2, label="FSST(0, 1, 1, 1)", color="blue", linewidth=1.0)
plt.plot(x, y3, label="FSST(0, 1, 1, 0.6)", color="green", linewidth=1.0)
plt.plot(x, y4, label="FSST(0, 1, 1, 0.8)", color="purple", linewidth=1.0)
plt.plot(x, y5, label="FSST(0, 1, 1, 2.0)", color="red", linewidth=1.0)

plt.xlabel("x")
plt.ylabel("Density")
plt.legend(loc="upper left")
plt.title("FSST CDF dengan Berbagai Nilai Alpha")
plt.show()

"""####Quantile"""

mu = pt.scalar('mu')
sigma = pt.scalar('sigma')
nu = pt.scalar('nu')
alpha = pt.scalar('alpha')
p = pt.scalar('p')

# Definisikan fungsi ICDF sebagai tensor operation
rv_quantile = fsst.icdf(p, mu, sigma, nu, alpha)
q = rv_quantile.eval({p: 0.9, mu: 0.0, sigma:1, nu: 1, alpha:2.0})
print(f"Quantile eval:", q)

# Kompilasi fungsi ICDF agar bisa dipakai dalam PyMC
rv_quantile_fn = pm.compile_pymc([p, mu, sigma, nu, alpha], rv_quantile)

# Uji dengan nilai tertentu
q_pymc = rv_quantile_fn(p=0.9, mu=0.0, sigma=1.0, nu=1, alpha=2.0)
print(f"Quantile dihitung dengan PyMC:", q_pymc)

"""#### Random"""

mu = 0.0
sigma = 1.0
nu = 1.0
alpha = 0.6

rv = fsst.dist(mu, sigma, nu, alpha)

hasilrandom = pm.draw(rv, draws=100)

# print(type(hasilrandom))
print(hasilrandom)
print(f"Mean: {np.mean(hasilrandom)}")
print(f"Variance: {np.var(hasilrandom)}")
print(f"Skewness: {scipy.stats.skew(hasilrandom)}")

import matplotlib.pyplot as plt
import seaborn as sns

# Pastikan cakupan x lebih luas
plt.figure(figsize=(12, 6))
x = np.linspace(-10, 10, 100)

# Plot KDE dari hasil random sampling
y1 = sns.kdeplot(hasilrandom, fill=True, bw_adjust=0.5, color="red", label='Data Bangkitan')

# Hitung PDF untuk distribusi FSST
y2 = [
    math.exp(rv_logp_fn(value=xi, mu=0, sigma=1, nu=2, alpha=1))  # nu=2 (df), alpha=1 (skewness)
    for xi in x
]

# Plot PDF distribusi FSST
plt.plot(x, y2, label='FSST(0, 1, 1, 2 )', color='blue', linewidth=1.0)

# Jika fill_between menutupi plot lain, coba hapus ini atau ubah alpha
plt.fill_between(x, y2, color="lightblue", alpha=0.3)

plt.legend()
plt.show()

"""####Coba pakai TFP"""

# Definisikan distribusi Two-Piece Student's t
tf.random.set_seed(42)
df = 1
loc = 0
scale = 1
skewness = 2
dist = tfd.TwoPieceStudentT(df=df, loc=loc, scale=scale, skewness=skewness)

# Hitung PDF teoritis
x = np.linspace(-5, 20, 100)
log_pdf = dist.log_prob(x).numpy()
pdf = np.exp(log_pdf)

#Bangkitkan sample
samples = dist.sample(100, seed=42).numpy()

print(f"Mean teoritis: {dist.mean().numpy()}")
print(f"Mode teoritis: {dist.mode().numpy()}")
print(f"Variance teoritis: {dist.variance().numpy()}")
print(f"======================================================")
print(f"Mean sample: {np.mean(samples)}")
print(f"Variance sample: {np.var(samples)}")
print(f"Skewness sample: {scipy.stats.skew(samples)}")

# Plot distribusi sampel (KDE)
plt.figure(figsize=(10, 6))
sns.kdeplot(samples, fill=True, color="red", label="Data Bangkitan FSST(0, 1, 1, 2)")

# Plot PDF teoritis
plt.plot(x, pdf, label="PDF teoritis FSST(0, 1, 1, 2)", color="blue", linewidth=1.5)
plt.fill_between(x, pdf, color="lightblue", alpha=0.5)

# Tambahkan label dan legenda
plt.xlabel("x")
plt.ylabel("Density")
plt.title("PDF Teoritis vs Data Bangkitan FSST(0, 1, 1, 2)")
plt.legend()
plt.show()

# Plot PDF teoritis
plt.figure(figsize=(12, 6))
plt.plot(x, pdf, color="blue", linewidth=1.5, label="PDF FSST(0, 1, 1, 2)")

# Tambahkan label dan legenda
plt.xlabel("x")
plt.ylabel("Density")
plt.title("PDF Teoritis FSST(0, 1, 1, 2)")
plt.legend()
plt.show()

"""### Validasi Output

#### PDF
"""

import pandas as pd
# Menentukan parameter yang akan digunakan
mu_grup = [-5, 0, 0.6, 9, 35]
sigma_grup = [0.1, 15, 30, 0.7]
alpha_grup = [0.4, 1, 17, 45]

# Nilai X yang akan dicari PDF dan CDFnya
x_grup = [-35.5, -5, -2.34, -10, 0, 20.22, 47, 13, 77, 5.76 ]

tabel_pdf = pd.DataFrame(columns=['X', 'mu', 'sigma', 'alpha', 'logpdf', 'PDF'])
tabel_cdf = pd.DataFrame(columns=['X', 'mu', 'sigma', 'alpha', 'logcdf', 'CDF'])

import random
for i in range(0,25):
    mu = random.choice(mu_grup)
    sigma = random.choice(sigma_grup)
    alpha = random.choice(alpha_grup)
    x = random.choice(x_grup)
    logpdf = rv_logp_fn(value=x, mu=mu, sigma=sigma, alpha=alpha)
    logcdf = rv_logcdf_fn(value=x, mu=mu, sigma=sigma, alpha=alpha)

    df_pdf = pd.DataFrame([{"X":x, "mu":mu, "sigma":sigma, "alpha":alpha, 'logpdf':np.round(logpdf, decimals=4), "PDF":np.round(math.exp(logpdf), decimals = 4)}])
    df_cdf = pd.DataFrame([{"X":x, "mu":mu, "sigma":sigma, "alpha":alpha, 'logcdf':np.round(logcdf, decimals=4), "CDF":np.round(math.exp(logcdf), decimals = 4)}])
    tabel_pdf = pd.concat([tabel_pdf, df_pdf], ignore_index=True)
    tabel_cdf = pd.concat([tabel_cdf, df_cdf], ignore_index=True)

tabel_pdf.to_csv('PDF.csv')
tabel_cdf.to_csv('CDF.csv')

from scipy.integrate import quad

def msnburr_pdf(x, mu, sigma, alpha):
  rv = msnburr.dist(mu,sigma,alpha)
  rv_logp = pm.logp(rv, x)
  return math.exp(rv_logp.eval())

def pembuktian1(mu, sigma, alpha):
  integral, error = quad(msnburr_pdf, -5, 10, args=(mu, sigma, alpha))
  return integral

def pembuktian2(mu, sigma, alpha):
  integral, error = quad(msnburr_pdf, -np.inf, 0.1, args=(mu, sigma, alpha))
  return integral

integral1 = pembuktian1(0, 1, 1)
integral2 = pembuktian2(0, 1, 1)
print(integral1)
print(integral2)

"""#### Random"""

def quantile(p, mu, sigma, alpha):
    omega = (1+(1/alpha))**(alpha+1)/np.sqrt(2*np.pi)
    q = mu - sigma/omega*(np.log(alpha)+np.log((p**(-1/alpha))-1))
    return q

p = quantile(p=0.7534389541609909,mu=0.0, sigma=10.0, alpha=1.0)
print(p)

"""### Validasi Hasil Estimasi Parameter

#### Bangkitkan Data
"""

data1 = msnburr.dist(mu=0, sigma=1, alpha=1)
data2 = msnburr.dist(mu=0, sigma=1, alpha=0.1)
data3 = msnburr.dist(mu=0, sigma=1, alpha=5)

hasilrandom = pm.draw(data3, draws=20000)

plt.hist(hasilrandom)
plt.show()

"""#### Buat Model"""

with pm.Model() as model1:
    mu = pm.Normal('mu',0,10)
    sigma = pm.HalfCauchy('sigma',10)
    alpha = pm.Gamma('alpha',2,0.1)

    msnburr.main(
        'MSNBurr',
        mu, sigma, alpha,
        hasilrandom
    )
with model1:
    trace_validasi = pm.sample(**sampler_kwargs)

az.plot_trace(trace_validasi)
az.summary(trace_validasi, round_to=3, hdi_prob=0.95)

"""#### Visualisasi"""

summary = az.summary(trace_validasi, round_to=3)
mean_values = summary['mean']
print(mean_values)

x = np.linspace(-5,15, 100)
y = [math.exp(rv_logp_fn(value=x, mu=mean_values[0], sigma=mean_values[1], alpha=mean_values[2])) for x in x]

fig, ax = plt.subplots(figsize=(8, 6))
ax.hist(hasilrandom, label='Data Simulasi', color='skyblue')
ax.set_xlabel("x")
legend = plt.legend(fontsize=13)
legend.set_bbox_to_anchor((0.775, 0.87), transform=plt.gcf().transFigure)

ax2 = ax.twinx()
ax2.plot(x, y, label='Hasil Estimasi Parameter', color='red', linewidth=1.5)
legend = plt.legend(fontsize=13)
legend.set_bbox_to_anchor((0.515, 0.81), transform=plt.gcf().transFigure)
plt.show()

"""# EVALUASI

## Prior
"""

import scipy.special as special
def skewness(alpha):
    skewness = (special.polygamma(2, alpha)-special.polygamma(2, 1))/(special.polygamma(1, alpha)+special.polygamma(1, 1))**(3/2)
    return skewness

x = np.linspace(0,15, 100)
y = [skewness(x) for x in x]
x1 = 10
y1 = skewness(x1)

# Membuat plot
plt.figure(figsize=(10, 6))
plt.plot(x, y, label='Skewness', color='blue')
plt.scatter(x1, y1, color='black')
plt.xlabel('Alpha')
plt.ylabel('Skewness')
plt.legend(fontsize=15)
plt.grid(True)
plt.show()

import math
import numpy as np
import matplotlib.pyplot as plt

value = pt.scalar("value")
rv = pm.Gamma.dist(alpha=2, beta=0.5)
rv_logp = pm.logp(rv, value)
rv_logp_fn = pm.compile_pymc([value], rv_logp)

x = np.linspace(0, 20, 1000)
y = [math.exp(rv_logp_fn(value=x)) for x in x]

# Plot density plot
plt.figure(figsize=(10, 5))
plt.plot(x, y, label=f'Gamma Distribution\nshape={2}, scale={0,5}', color='blue')
plt.title('Gamma Distribution wtih PyMC')
plt.xlabel('x')
plt.ylabel('Probability Density')
plt.legend()
plt.grid(True)
plt.show()

from scipy.stats import halfcauchy
x = np.linspace(0, 200, 1000)
y = halfcauchy.pdf(x, scale=30)

# Membuat plot
plt.figure(figsize=(8, 4))
plt.plot(x, y, label='Half Cauchy Distribution\nscale=5', color='red')
plt.title('Half Cauchy Distribution')
plt.xlabel('x')
plt.ylabel('Probability Density')
plt.legend()
plt.grid(True)
plt.show()

"""## AHH

### Histogram
"""

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_excel("AHH Indo 2023.xlsx")
data = df.iloc[:, 1]
plt.hist(data)
plt.xlabel("AHH")
plt.ylabel("Frekuensi")
plt.title("Histogram AHH Tahun 2023")
plt.show()

"""### Uji Normalitas dan Kemencengan

"""

from scipy import stats

print("AHH Tahun 2023")
# Normality test (Shapiro-Wilk test)
shapiro_test_result = stats.shapiro(data)
print(f"Shapiro-Wilk test (p-value): {shapiro_test_result[1]}")
# Skewness test
skewness = stats.skew(data)
print(f"Skewness: {skewness}")

"""### Estimasi Parameter"""

with pm.Model() as modelNormal:
    mu = pm.Normal('mu',74,5)
    sigma = pm.HalfCauchy('sigma',5)

    pm.Normal('Regresi Normal', mu=mu, sigma=sigma, observed=data)
    traceNormal = pm.sample(**sampler_kwargs)

az.plot_trace(traceNormal)
az.summary(traceNormal, round_to=3, hdi_prob=0.95)

with pm.Model() as modelAHH:
    mu = pm.Normal('mu',74,5)
    sigma = pm.HalfCauchy('sigma',5)
    alpha = pm.Gamma('alpha',2,0.5)

    msnburr.main(
        'MSNBurr',
        mu, sigma, alpha,
        data
    )
    traceMSNBurr = pm.sample(**sampler_kwargs)

az.plot_trace(traceMSNBurr)
az.summary(traceMSNBurr, round_to=3, hdi_prob=0.95)

"""### Karakteristik Data"""

summary1 = az.summary(traceNormal, round_to=3)
summary2 = az.summary(traceMSNBurr, round_to=3)
mean_values_normal = summary1['mean']
mean_values = summary2['mean']
print("Parameter Distribusi Normal")
print(mean_values_normal)
print("Parameter Distribusi MSNBurr")
print(mean_values)

import scipy.special as special
def expected_value(mu, sigma, alpha):
    omega = (1+(1/alpha))**(alpha+1)/np.sqrt(2*np.pi)
    expected_value = mu + sigma/omega*(special.polygamma(0, alpha)-special.polygamma(0, 1)-np.log(alpha))
    return expected_value
def variance(sigma, alpha):
    omega = (1+(1/alpha))**(alpha+1)/np.sqrt(2*np.pi)
    variance = (sigma**2)/(omega**2)*(special.polygamma(1, alpha)+special.polygamma(1,1))
    return variance
def skewness(alpha):
    skewness = (special.polygamma(2, alpha)-special.polygamma(2, 1))/(special.polygamma(1, alpha)+special.polygamma(1, 1))**(3/2)
    return skewness

mu = mean_values[0]
sigma = mean_values[1]
alpha = mean_values[2]

expected_value_ = expected_value(mu, sigma, alpha)
variance_ = variance(sigma, alpha)
mode_ = mu
skewness_ = skewness(alpha)

print("Expected value (E(x)):", expected_value_)
print("Variance (Var(x)):", variance_)
print("Mode:", mode_)
print("Skewness:", skewness_)

"""### Visualisasi

"""

mu = pt.scalar('mu')
sigma = pt.scalar('sigma')
alpha = pt.scalar('alpha')
value = pt.scalar('value')

rv = msnburr.dist(mu=mu, sigma=sigma, alpha=alpha)
rv_logp = pm.logp(rv, value)
rv_logp_fn = pm.compile_pymc([value, mu, sigma, alpha], rv_logp)

from scipy.stats import norm
x = np.linspace(50, 83, 100)
y = [math.exp(rv_logp_fn(value=xi, mu=mean_values[0], sigma=mean_values[1], alpha=mean_values[2])) for xi in x]
y1 = [norm.pdf(x, mean_values_normal[0], mean_values_normal[1] ) for x in x]

fig, ax = plt.subplots(figsize=(8, 6))
ax.hist(data, label='Data Asli', color='skyblue')

ax2 = ax.twinx()
ax2.plot(x, y1, label='Hasil Estimasi Normal', color='green', linewidth=2)
ax2.plot(x, y, label='Hasil Estimasi MSNBurr', color='red', linewidth=2)
ax2.set_ylim([0, max(y) * 1.1])

ax.set_xlabel("x")
ax.set_ylabel("Density")
ax2.set_ylabel("Estimated Density")
lines, labels = ax.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax2.legend(lines + lines2, labels + labels2, loc='upper left', fontsize=13)

plt.show()

"""## Simulasi Regresi"""

x1 = np.random.normal(0,1,20000)
x2 = np.random.normal(0,1,20000)
b0 = 0.63
b1 = 0.78
b2 = 0.003
ycap = b0 + b1*x1 + b2*x2
eps1 = np.random.normal(0,1,20000)
eps2 = pm.draw(msnburr.dist(0,1,1), draws=20000)
eps3 = pm.draw(msnburr.dist(0,1,0.1), draws=20000)
eps4 = pm.draw(msnburr.dist(0,1,5), draws=20000)
y = ycap + eps4

with pm.Model() as simulasiregresi:
    sigma = pm.HalfCauchy('sigma',5)
    alpha = pm.Gamma('alpha',2,0.5)
    b0 = pm.Normal('b0',0, 0.5)
    b1 = pm.Normal('b1',0, 0.5)
    b2 = pm.Normal('b2',0, 0.5)

    eps = b0 + b1*x1 + b2*x2

    msnburr.main('Simulasi Regresi', mu=eps, sigma=sigma, alpha=alpha, observed=y)

with simulasiregresi:
    trace_coba = pm.sample(**sampler_kwargs)

az.plot_trace(trace_coba)
az.summary(trace_coba, round_to=3, hdi_prob=0.95)

"""## PDRB

### Load Data
"""

import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
import matplotlib.pyplot as plt
from scipy.stats import skew
from sklearn.metrics import mean_squared_error

data1 = pd.read_excel("DATA.xlsx")
data = data1.iloc[:,1:3]
data.head()

y = np.log(data["Y"])
x1 = np.log(data["X1"])

"""### Cek dengan OLS

"""

# 1. Mengimpor Pustaka
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from scipy.stats import skew
from sklearn.metrics import mean_squared_error

# 2. Menyiapkan Data
X = np.log(data[["X1"]])
Y = np.log(data["Y"])

# 3. Menambahkan Kolom Konstan
X = sm.add_constant(X)

# 4. Membuat Model Regresi OLS dan Melakukan Fitting
model = sm.OLS(Y, X).fit()

# 5. Melihat Ringkasan Hasil Regresi
print(model.summary())

# 6. Menghitung Residual
residuals = model.resid

y_pred = model.predict(X)
# 7. Menghitung Mean Squared Error (MSE)
mse = mean_squared_error(Y, y_pred)
print(f'Mean Squared Error (MSE): {mse}')

plt.hist(residuals, edgecolor='black')  # Customize bins, color, and transparency
plt.title('Histogram of Residuals')
plt.show()

"""### Uji Normalitas"""

import scipy.stats as stats
shapiro_test_result = stats.shapiro(data)
print(f"Shapiro-Wilk test (p-value): {shapiro_test_result[1]}")

"""### Uji Linieritas"""

import rpy2.robjects as ro
from rpy2.robjects.packages import importr
from rpy2.robjects.vectors import FloatVector
import rpy2.robjects.numpy2ri

utils = importr('utils')
utils.install_packages('tseries')

# Aktifkan konversi numpy
rpy2.robjects.numpy2ri.activate()

# Impor paket tseries dari R
tseries = importr('tseries')

# Convert numpy arrays to R vectors
x1_r = FloatVector(x1)
# x2_r = FloatVector(x2)
# x3_r = FloatVector(x3)
y_r = FloatVector(y)

# Gabungkan x1, x2, x3 menjadi matriks di R
# x_combined_r = ro.r['cbind'](x1_r, x2_r)

# Menjalankan Terasvirta Test
# terasvirta_test_result = tseries.terasvirta_test(x_combined_r, y_r)
terasvirta_test_result = tseries.terasvirta_test(x1_r,y_r)

# Mendapatkan hasil
f_statistic = terasvirta_test_result.rx2('statistic')[0]
p_value = terasvirta_test_result.rx2('p.value')[0]

# Menampilkan hasil
print(f'F-statistic: {f_statistic}, p-value: {p_value}')

df = pd.DataFrame(data)

# Scatter plot X1 vs Y
plt.scatter(df['X1'], df['Y'])
plt.xlabel('X1')
plt.ylabel('Y')
plt.title('Scatter plot of X1 vs Y')
plt.show()

# Pair plot untuk semua variabel
sns.pairplot(df)
plt.show()

# Correlation matrix
correlation_matrix = df.corr()
print(correlation_matrix)

"""### Regresi Bayesian Normal

#### Build Model
"""

with pm.Model() as regresinormal:
    # Prior
    sigma = pm.HalfCauchy('sigma',5)
    b0 = pm.Normal('b0',0, 5)
    b1 = pm.Normal('b1',0, 1)
    # b2 = pm.Normal('b2',0, 0.5)

    eps = b0 + b1*x1

    pm.Normal('Regresi Normal', mu=eps, sigma=sigma, observed=y)
    step = pm.NUTS(target_accept=0.95, max_treedepth=15)
    trace = pm.sample(3000, step=step, chains=4, cores=4,return_inferencedata=True, random_seed=42, tune=2000)

az.plot_trace(trace)

az.summary(trace, round_to=5, hdi_prob=0.95)

summary = az.summary(trace, round_to=3)
mean_values = summary['mean']
print("Nilai mean dari setiap variabel:")
print(mean_values)

"""#### Posterior Predictive Check"""

with regresinormal:
    # Draw samples from posterior predictive
    post_pred = pm.sample_posterior_predictive(trace)
    trace.extend(post_pred)

Y = np.log(data["Y"])
fig, ax = plt.subplots()
x = xr.DataArray(np.linspace(0, 34, 34), dims=["plot_dim"])
y = trace.posterior_predictive['Regresi Normal']

ax.plot(x, y.stack(sample=("chain","draw")).values.squeeze(), c="k", alpha=0.4)
ax.plot(Y, color="r", label="data")
ax.legend();

y = np.log(data["Y"])
x1 = np.log(data["X1"])
y_pred = mean_values[0] + mean_values[1]*x1
residuals = y - y_pred
plt.hist(residuals, edgecolor='black')  # Customize bins, color, and transparency
plt.title('PDRB')
plt.xlabel('Nilai')
plt.ylabel('Frekuensi')
plt.show()

"""### Regresi Bayesian MSNBurr

#### Build Model
"""

y = np.log(data["Y"])
with pm.Model() as regresimsnburr:
    sigma = pm.HalfCauchy('sigma',5)
    alpha = pm.Gamma('alpha',2,0.5)
    b0 = pm.Normal('b0',0, 5)
    b1 = pm.Normal('b1',0, 1)

    eps = b0 + b1*x1

    msnburr.main('Regresi MSNBurr', mu=eps, sigma=sigma, alpha=alpha, observed=y)
    step = pm.NUTS(target_accept=0.95, max_treedepth=15)
    trace2 = pm.sample(3000, step=step, chains=4, cores=4,return_inferencedata=True, random_seed=42, tune=2000)

az.plot_trace(trace2)

az.summary(trace2, round_to=5, hdi_prob=0.95)

summary = az.summary(trace, round_to=3)
mean_values = summary['mean']
print("Nilai mean dari setiap variabel:")
print(mean_values)

"""#### Posterior Predictive Check"""

with regresimsnburr:
    # Draw samples from posterior predictive
    post_pred = pm.sample_posterior_predictive(trace2)
    trace2.extend(post_pred)

Y = np.log(data["Y"])
fig, ax = plt.subplots()
x = xr.DataArray(np.linspace(0, 34, 34), dims=["plot_dim"])
y = trace2.posterior_predictive['Regresi MSNBurr']

ax.plot(x, y.stack(sample=("chain","draw")).values.squeeze(), c="k", alpha=0.4)
ax.plot(Y, color="r", label="data")
ax.legend();

y = np.log(data["Y"])
x1 = np.log(data["X1"])
y_pred = mean_values[0] + mean_values[1]*x1
residuals = y - y_pred
plt.hist(residuals, edgecolor='black')  # Customize bins, color, and transparency
plt.title('PDRB ADHK')
plt.xlabel('Nilai')
plt.ylabel('Frekuensi')
plt.show()

"""### Cek Kebaikan Model"""

with regresinormal:
    pm.compute_log_likelihood(trace)

waic = az.waic(trace)
loo = az.loo(trace)

print(waic)
print(loo)

with regresimsnburr:
    pm.compute_log_likelihood(trace2)

waic = az.waic(trace2)
loo = az.loo(trace2)

print(waic)
print(loo)